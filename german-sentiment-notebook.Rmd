---
title: "R Notebook"
output: html_notebook
---

## Dataset Paths & Reading TSV Files

```{r}
# SentiWS provides a file for positive scores and negative scores separately
sentiws_neg_path <- "data/SentiWS/SentiWS_v2.0_Negative.txt"
sentiws_pos_path <- "data/SentiWS/SentiWS_v2.0_Positive.txt"

# create a simple function to read in tab-separated value formats
read_tsv <- function(path, cols) {
  table <- read.table(path, 
                      header = TRUE, 
                      sep = "\t", 
                      stringsAsFactors = FALSE,
                      fill = TRUE,
                      row.names = NULL,
                      col.names = cols)
  print(table)
  return (table)
}
```

## Load SentiWS Positive & Negative Datasets

```{r}
load_sentiws <- function(path) {
  cols <- c("lexical_item", "sent_score", "alt_forms")
  sentiws_data <- read_tsv(path, cols)
  
  # we first need to split the lexical items and POS tags
  # since they are initially provided in the same column
  item_and_pos_col <- sentiws_data$lexical_item
  item_pos_split <- lapply(item_and_pos_col, 
                           function(x) strsplit(x, split="\\|")[[1]])
  
  # create a column for the items and the pos tags
  lexical_items <- sapply(item_pos_split, function(x) x[1])
  pos_tags <- sapply(item_pos_split, function(x) x[2])
  
  # update the columns with item and pos tag
  sentiws_data["lexical_item"] = lexical_items
  sentiws_data["pos"] = pos_tags
  
  return (sentiws_data)
}

# load the positive and negative SentiWS sentiment scores
# as a dataframe with some preprocessing
df_sentiws_pos <- load_sentiws(sentiws_pos_path)
df_sentiws_neg <- load_sentiws(sentiws_neg_path)

df_sentiws_neg[1:5,]
df_sentiws_pos[1:5,]

```

## Load Treebank Data

```{r}
load_treebank <- function(path, cols) {
  treebank_cols <- c("id", "left_context", "pivot", "right_context")
  treebank_data <- read_tsv(treebank_data_path, treebank_cols)
  treebank_data$full_sentence <- paste(
    treebank_data$left_context, 
    treebank_data$pivot, 
    treebank_data$right_context,
    sep = " ")
  return(treebank_data)
}
```

## Preprocessing & Creation of Document-Term Matrix

```{r}
library(tm)
create_document_term_matrix <- function(sentence, language) {
  # VCorpus in tm refers to "Volatile" corpus which means that the corpus is stored in memory and would be destroyed when the R object containing it is destroyed.
  # contrast this with PCorpus or Permanent Corpus which are stored outside the memory in a db.
  corpus <- VCorpus(VectorSource(sentences))
  
  # preprocessing: remove whitespace, punctuation, and stopwords
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeWords, stopwords(language))
  return (
    DocumentTermMatrix(corpus, control = list(tolower = FALSE))
  )
}
```

## Sentence-Term Frequency Matrix

```{r}
# the columns with an alternation will be added to the original base word
# TODO: the columns with no distinction between upper and lower will be added together

combine_forms <- function(dtm_forms, lexical_items, alt_forms) {
  all_terms <- Terms(dtm_forms)
  dtm_forms <- as.matrix(dtm_forms)
  lexical_items <- lexical_items
  
  base_words_in_dtm <- lapply(lexical_items, function(item) item %in% all_terms)
  
  for (i in 1:length(lexical_items)) {
    # check if it contains the lexical_item
    lex <- lexical_items[i]
    alternatives <- alt_forms[i]
    alternatives <- unlist(alternatives)
    contains_lexical_item <- base_words_in_dtm[[i]]
  
    # get a boolean list of which alternate forms for lexical item i appear in the DTM
    contains_alt_forms <- lapply(alternatives, function(x) x %in% all_terms)
    contains_alt_forms <- unlist(contains_alt_forms)
    
    # isolate only those alternate forms which appear
    contained_alt_forms <- alternatives[contains_alt_forms]
    
    # first we'll concatenate the columns into the one base word column
    # since they will all have the same sentiment value
    if (!contains_lexical_item) {
      if (length(contained_alt_forms) == 0) {
        next
      } 
      else {
        empty_col <- rep(0, nrow(dtm_forms))
        dtm_forms <- cbind(dtm_forms, empty_col)
        colnames(dtm_forms)[ncol(dtm_forms)] <- lex
      }
    }
    
    if (length(contained_alt_forms) > 1) {
        alternate_form_sum <- rowSums(dtm_forms[, contained_alt_forms])
        dtm_forms[, lex] <- 
        if (!contains_lexical_item) {
            alternate_form_sum
        } 
        else {
            dtm_forms[, lex] + alternate_form_sum
        }
    } 
    else if (length(contained_alt_forms) == 1) {
      dtm_forms[, lex] <- 
        if (!contains_lexical_item) {
            dtm_forms[, contained_alt_forms[1]]
        } 
        else {
            dtm_forms[, lex] + dtm_forms[, contained_alt_forms[1]]
        }
    }
    
    # then we remove the original alternative form columns from the table
    dtm_forms <- dtm_forms[, !colnames(dtm_forms) %in% contained_alt_forms]
    
  }
  # then we remove all the words which are not base words in the sentiment
  # dataset as they will not contribute to the final score
  # behaves strangely if only one column returned
  dtm_forms <- dtm_forms[, colnames(dtm_forms) %in% lexical_items]
  return (dtm_forms)
}

```

## Column-wise Matrix Multiplication of Sentiment Scores

```{r}
apply_sentiment_scores <- function(dtm_forms, lexical_items, sentiment_scores) {
  cols <- colnames(dtm_forms)
  required_scores <- lapply(lexical_items, function(x) x %in% colnames(dtm_forms))
  required_scores <- unlist(required_scores)
  required_scores <- sentiment_scores[required_scores]
  
  # apply them colwise
  dtm_forms <- dtm_forms %*% diag(required_scores)
  colnames(dtm_forms) <- cols
  return (dtm_forms)
}
```

## Run sentiment analysis

```{r}
# A matrix with each sentence as a row and each term as a column
# and the values representing the sentiment provided by that term
# for the given sentence
# if a term occurs multiple times in a sentence, the value corresponds to:
# (# occurrences) x (sentiment score)
get_sentiment_matrix <- function(dtm, df_sentiws) {
  lexical_items <- unlist(df_sentiws$lexical_item)

  # check which alternate forms it contains - should be a vector
  alt_forms <- df_sentiws$alt_forms
  alt_forms_processed <- lapply(alt_forms, 
                                function(x) strsplit(x, split = ","))
  alt_forms_processed <- unlist(alt_forms_processed)
  
  final_dtm <- combine_forms(dtm, 
                             lexical_items, 
                             alt_forms_processed)
  
  # SENTIMENT 
  sentiment_scores <- unlist(df_sentiws$sent_score)
  sentiment_matrix <- apply_sentiment_scores(final_dtm, 
                                             lexical_items, 
                                             sentiment_scores)
  
  return (sentiment_matrix)
}

# run the algorithm for both the negative and positive
# Leipzig SentiWS datasets and combine them to get the
# final sentiment score for each sentence.
run_sentiment_analysis <- function(sentences, df_sentiws_neg, df_sentiws_pos) {
  
  dtm <- create_document_term_matrix(sentences, lang="german")
  
  sentiment_matrix_neg <- get_sentiment_matrix(dtm, df_sentiws_neg)
  sentiment_matrix_pos <- get_sentiment_matrix(dtm, df_sentiws_pos)
  
  # sum together the rows of the matrix to get the sentiments
  sentence_sentiments_neg <- rowSums(sentiment_matrix_neg)
  sentence_sentiments_pos <- rowSums(sentiment_matrix_pos)
  return (sentence_sentiments_neg + sentence_sentiments_pos)
}

# TODO: Run on whole dataset
# TODO: Find out which proper nouns have the lowest sentiment scores
```

```{r}
treebank_data_path <- "data/propn-grew-query-results.tsv"
treebank_data <- load_treebank(treebank_data_path, treebank_cols)
n_sentences <- 500
sentences <- as.vector(treebank_data[1:n_sentences, 'full_sentence'])

sentiments <- run_sentiment_analysis(sentences, df_sentiws_neg, df_sentiws_pos)

print("POSITIVE:")
most_positive_indices <- which.max(sentiments)
for (i in most_positive_indices) {
  print(paste(sentences[i], sentiments[i], sep = " -- "))
}

print("NEGATIVE:")
most_negative_indices <- which.min(sentiments)
for (i in most_negative_indices) {
  print(paste(sentences[i], sentiments[i], sep = " -- "))
}

# initialize a new column with NA values
treebank_data$sentiment <- NA  # This will create a new column with all NA values

# assign the new values to the first few rows
treebank_data[1:length(sentiments), "sentiment"] <- sentiments

head(treebank_data)
```

---
title: "German Sentiment Notebook"
output: html_notebook
---

# Sentiment of Named Entities in German

## Batch Sentiment Analysis of German Using SentiWS & Matrices

**Author:** Daniel Gallagher

This project builds a batch sentiment analyser for German based on the Leipzig SentiWS sentiment resource. Testing is performed on a subset of the UD German GSD treebank in order to determine sentiment towards particular named entities.

**Leipzig SentiWS Dataset**

This project uses the Leipzig SentiWS dataset to perform semantic analysis of German sentences. SentiWS is a publicly-available resource for calculating German sentiment. It provides a dataset of German terms and their assigned sentiment score, which can range from -1 (very negative sentiment) to +1 (very positive sentiment). It additionally lists their part-of-speech tag as well as any alterations of that word that can appear. SentiWS was created by Remus et al. at the NLP department of the University of Leipzig.

**UD German GSD**

The sentence data used for testing was collected from the UD German GSD treebank. A query was run using the *Grew* query language which extracted any sentences which contained:

-   Any one-word named entities which did not have a determiner or number modifying them.

The query can be seen below:

``` sql
pattern { X [NamedEntity=Yes, upos="PROPN"]; }
without { X -[det|nummod]-> Y }
without { Z -[flat]-> X }
```

**Batch Sentiment Analysis**

The term 'batch' refers to the fact that this tool can carry out sentiment analysis across many samples at once rather than running on each sentence individually. This is carried out through the creation of sparse sentiment matrices and their manipulation in order to find the final sentiment scores for each sentence.

**Research Question**

As we have seen, the treebank data that we have collected has isolated named entities in German. The features of this dataset include *left_context, pivot, right_context*. The *pivot* here is the named entity, and that is surrounded by the left and right context. Once we batch sentiment analysis across the dataset we will have sentiment scores for each sentence. The purpose of this setup is to find the named entities which are mentioned in the most positive and most negative contexts. While this dataset will be a small sample of only 260 sentences, it will allow us to test the sentiment analyser as well as provide some basic insights into this question. This can be particularly useful for performing large sentiment analysis across categories to determine public opinion towards a particular entity.

## Dataset Paths & Reading TSV Files

We will initially start with a few basics. We will be working with the TSV instead of CSV format and will therefore require a function which allows us to read this in.

```{r}
# SentiWS provides a file for positive scores and negative scores separately
sentiws_neg_path <- "data/SentiWS/SentiWS_v2.0_Negative.txt"
sentiws_pos_path <- "data/SentiWS/SentiWS_v2.0_Positive.txt"

# create a simple function to read in tab-separated value formats
read_tsv <- function(path, cols) {
  table <- read.table(path, 
                      header = TRUE, 
                      sep = "\t", 
                      stringsAsFactors = FALSE,
                      fill = TRUE,
                      row.names = NULL,
                      col.names = cols)
  return (table)
}
```

## Load SentiWS Positive & Negative Datasets

The Leipzig SentiWS dataset comes with a lexical item and its POS tag, a sentiment score, and the alternate forms that a lexical item takes. We will perform some preprocessing to separate the lexical item and its POS tag into separate columns.

```{r}
load_sentiws <- function(path) {
  cols <- c("lexical_item", "sent_score", "alt_forms")
  sentiws_data <- read_tsv(path, cols)
  
  # we first need to split the lexical items and POS tags
  # since they are initially provided in the same column
  item_and_pos_col <- sentiws_data$lexical_item
  item_pos_split <- lapply(item_and_pos_col, 
                           function(x) strsplit(x, split="\\|")[[1]])
  
  # create a column for the items and the pos tags
  lexical_items <- sapply(item_pos_split, function(x) x[1])
  pos_tags <- sapply(item_pos_split, function(x) x[2])
  
  # update the columns with item and pos tag
  sentiws_data["lexical_item"] = lexical_items
  sentiws_data["pos"] = pos_tags
  
  return (sentiws_data)
}

# load the positive and negative SentiWS sentiment scores
# as a dataframe with some preprocessing
df_sentiws_pos <- load_sentiws(sentiws_pos_path)
df_sentiws_neg <- load_sentiws(sentiws_neg_path)

df_sentiws_neg[1:5,]
df_sentiws_pos[1:5,]

```

## Load Treebank Data

The sentence data for this project has been taken from the **UD German GSD** treebank. The following query was run in order to isolate any sentences which contained proper nouns:

``` sql
pattern { X [NamedEntity=Yes, upos="PROPN"]; }
without { X -[det|nummod]-> Y }
without { Z -[flat]-> X }
```

This query found any one-word named entities which did not have a determiner or number modifying them. The results were then saved in a TSV format and can be loaded using the following function.

```{r}
load_treebank <- function(path, cols) {
  treebank_cols <- c("id", "left_context", "pivot", "right_context")
  treebank_data <- read_tsv(treebank_data_path, treebank_cols)
  treebank_data$full_sentence <- paste(
    treebank_data$left_context, 
    treebank_data$pivot, 
    treebank_data$right_context,
    sep = "")
  return(treebank_data)
}
```

## Preprocessing & Creation of Document-Term Matrix

A document-term matrix is created using the *tm* package. First, a VCorpus (volatile corpus) is created. This is a corpus which is only stored temporarily until the R object is destroyed. Preprocessing is carried out which:

-   Strips whitespace.

-   Removes punctuation.

-   Removes stopwords.

A document-term matrix is then created.

```{r}
library(tm)
create_document_term_matrix <- function(sentence, 
                                        language) {
  # VCorpus in tm refers to "Volatile" corpus which means that the corpus is stored in memory and would be destroyed when the R object containing it is destroyed.
  # contrast this with PCorpus or Permanent Corpus which are stored outside the memory in a db.
  corpus <- VCorpus(VectorSource(sentences))
  
  # preprocessing: remove whitespace, punctuation, and stopwords
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeWords, stopwords(language))
  return (
    DocumentTermMatrix(corpus, control = list(tolower = FALSE))
  )
}
```

## Sentence-Term Frequency Matrix

Columns (i.e terms) in the Document-Term matrix will be manipulated based on the following criteria:

-   If a sentence doesn't contain a lexical item or its alterations, move on.
-   If a sentence doesn't contain a lexical item but contains one or more alterations, initialise a zero column for the lexical item and set it equal to the sum of any alterations in a sentence.
-   If a sentence contains both a lexical item and one or more alterations, sum the alterations and append them to the lexical item column.
-   Finally, remove any columns from the DTM in which the term is not semantically relevant. That is, it does not occur in the SentiWS dataset.

The purpose of these rules is to end up with a Document-Term Matrix which contains only the lexical items of the Leipzig SentiWS dataset and not any alterations. This is because the alterations of a lexical item should be treated the same as the lexical item itself. For instance, the lexical item "Abschluss" will be the final column in the matrix and any sentiments of alterations such as "Abschlusses" will be added to the lexical item column. These columns with alterations will then be removed.

```{r}
# calculate sum for specified columns
calculate_column_sums <- function(m, col_names) {
  selected_cols <- m[, col_names, drop = FALSE]
  
  if (ncol(selected_cols) > 1) {
    result <- apply(selected_cols, 1, sum)
  } else {
    result <- selected_cols
  }
  
  return(result)
}

combine_forms <- function(dtm_forms, lexical_items, alt_forms) {
  all_terms <- Terms(dtm_forms)
  dtm_forms <- as.matrix(dtm_forms)
  
  base_words_in_dtm <- lapply(lexical_items, function(item) item %in% all_terms)
  
  for (i in 1:length(lexical_items)) {
    # check if it contains the lexical_item
    lex <- lexical_items[i]
    alternatives <- alt_forms[i]
    alternatives <- unlist(alternatives)
    lexical_item_in_dtm <- base_words_in_dtm[[i]]
  
    # get a boolean list of which alternate forms 
    # for lexical item i appear in the DTM
    contains_alt_forms <- lapply(alternatives, function(x) x %in% all_terms)
    contains_alt_forms <- unlist(contains_alt_forms)
    
    # isolate only those alternate forms which appear
    contained_alt_forms <- alternatives[contains_alt_forms]
    
    # first we'll concatenate the columns into the one base word column
    # since they will all have the same sentiment value
    n_alt_forms_in_dtm <- length(contained_alt_forms)
    if (!lexical_item_in_dtm && n_alt_forms_in_dtm == 0) {
        next
      } 
    else if(!lexical_item_in_dtm) {
        empty_col <- rep(0, nrow(dtm_forms))
        dtm_forms <- cbind(dtm_forms, empty_col)
        colnames(dtm_forms)[ncol(dtm_forms)] <- lex
      }
      # otherwise the lexical item is in the DTM
      # and we don't need to create a column
    
    if (n_alt_forms_in_dtm >= 1) {
      # does the following line work?
        alternate_form_sum <- calculate_column_sums(dtm_forms, 
                                                    contained_alt_forms)
        
        dtm_forms[, lex] <- dtm_forms[, lex] + alternate_form_sum
      } 
    
      # then we remove the original alternative form columns from the table
      dtm_forms <- dtm_forms[, !colnames(dtm_forms) %in% contained_alt_forms]
  }
  # then we remove all the words which are not base words in the sentiment
  # dataset as they will not contribute to the final score
  # behaves strangely if only one column returned
  dtm_forms <- dtm_forms[, colnames(dtm_forms) %in% lexical_items]
  return (dtm_forms)
}

```

## Column-wise Matrix Multiplication of Sentiment Scores

The application of our sentiment scores to the matrix which contains each sentence and the Leipzig SentiWS semantically-relevant terms is carried out by a matrix multiplication of the matrix with the diagonal of the relevant sentiment scores.

```{r}
apply_sentiment_scores <- function(dtm_forms, 
                                   lexical_items, 
                                   sentiment_scores) {
  cols <- colnames(dtm_forms)
  required_scores <- lapply(lexical_items, function(x) x %in% cols)
  required_scores <- unlist(required_scores)
  required_scores <- sentiment_scores[required_scores]
  
  dtm_forms <- dtm_forms %*% diag(required_scores)
  colnames(dtm_forms) <- cols
  return (dtm_forms)
}
```

## Sentiment Matrix

The final sentiment matrix corresponds to

-   (sentence, sentiment-relevant term) = sentiment score

where the sentiment score is:

-   (\# sentence occurrences) x (Leipzig SentiWS sentiment score for a given term)

For instance, take an example sentiment matrix below. We see that the rows correspond to each sentence and the columns correspond to any semantically-relevant term. That is because any terms which do not contribute to the sentiment score for at least one of the sentences has been purged. The value corresponds to the sentiment provided by that word for the given sentence. This matrix will be very sparse as each sentence will only use a small subset of the words.

|            | Abstimmung | akzeptieren | Angebot | angenehm  | Anspruch  |
|------------|------------|-------------|---------|-----------|-----------|
| Sentence A | 1 x 0.004  | 0           | 0       | 0         | 0         |
| Sentence B | 0          | 0           | 0       | 1 x 0.004 | 0         |
| Sentence C | 0          | 2 x 0.004   | 0       | 0         | 0         |
| Sentence D | 0          | 0           | 0       | 1 x 0.004 | 1 x 0.004 |

The columns of the sentiment matrix can then be summed together to get the final contribution of the semantically-relevant terms to each sentence. This leaves us with a vector rather than a matrix.

```{r}
# A matrix with each sentence as a row and each term as a column
# and the values representing the sentiment provided by that term
# for the given sentence
# if a term occurs multiple times in a sentence, the value corresponds to:
# (# occurrences) x (sentiment score)
get_sentiment_matrix <- function(dtm, df_sentiws) {
  lexical_items <- unlist(df_sentiws$lexical_item)

  # check which alternate forms it contains - should be a vector
  alt_forms <- df_sentiws$alt_forms
  alt_forms_processed <- lapply(alt_forms, 
                                function(x) strsplit(x, split = ","))
  alt_forms_processed <- unlist(alt_forms_processed)
  
  final_dtm <- combine_forms(dtm, 
                             lexical_items, 
                             alt_forms_processed)
  
  # SENTIMENT 
  sentiment_scores <- unlist(df_sentiws$sent_score)
  sentiment_matrix <- apply_sentiment_scores(final_dtm, 
                                             lexical_items, 
                                             sentiment_scores)
  
  return (sentiment_matrix)
}

# run the algorithm for both the negative and positive
# Leipzig SentiWS datasets and combine them to get the
# final sentiment score for each sentence.
run_sentiment_analysis <- function(sentences, df_sentiws_neg, df_sentiws_pos) {
  
  dtm <- create_document_term_matrix(sentences, lang="german")
  
  sentiment_matrix_neg <- get_sentiment_matrix(dtm, df_sentiws_neg)
  sentiment_matrix_pos <- get_sentiment_matrix(dtm, df_sentiws_pos)
  
  # sum together the rows of the matrix to get the sentiments
  sentence_sentiments_neg <- rowSums(sentiment_matrix_neg)
  sentence_sentiments_pos <- rowSums(sentiment_matrix_pos)
  return (sentence_sentiments_neg + sentence_sentiments_pos)
}
```

## Run Sentiment Analysis

We can now combine everything that we've worked on so far to run the sentiment analysis on our German sentence dataset.

```{r}
treebank_data_path <- "data/propn-grew-query-results.tsv"
treebank_data <- load_treebank(treebank_data_path, treebank_cols)

print(head(treebank_data$full_sentence))
print(paste("Number of sentences: ", nrow(treebank_data)))
treebank_data

# convert sentences to string vector
n_sentences <- 260
sentences <- as.vector(treebank_data$full_sentence[1:n_sentences])
```

```{r}

sentiments <- run_sentiment_analysis(sentences, df_sentiws_neg, df_sentiws_pos)

print("POSITIVE:")
most_positive_indices <- which.max(sentiments)
for (i in most_positive_indices) {
  print(paste(sentences[i], sentiments[i], sep = " -- "))
}

print("NEGATIVE:")
most_negative_indices <- which.min(sentiments)
for (i in most_negative_indices) {
  print(paste(sentences[i], sentiments[i], sep = " -- "))
}

# initialize a new column with NA values
treebank_data$sentiment <- 0  # This will create a new column with all NA values

# assign the new values to the first few rows
treebank_data[1:length(sentiments), "sentiment"] <- sentiments

View(treebank_data)
```

## Plotting the Results

We will view the proper nouns which are referenced in the top 15 most positive and negative sentences. These will be plotted using GGPlot.

```{r}
library(ggplot2)

# order dataset based on sentiment score
df_sorted <- treebank_data[order(treebank_data$sentiment), ]

# subset into the most positive and most negative
n_results <- 15
top_10 <- tail(df_sorted, n_results)
bottom_10 <- head(df_sorted, n_results)
df_top_bottom <- rbind(bottom_10, top_10)

# plot the results
ggplot(df_top_bottom, 
       aes(x = reorder(pivot, sentiment), 
                          y = sentiment, 
                          fill = sentiment > 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_classic() +
  labs(title = "Top 10 Highest and Lowest Sentiment Scores",
       x = "Proper Noun",
       y = "Sentiment Score") + 
  theme(legend.position = "none") +
  ylim(-1.2,1.2)

```

We may be interested to see the entire sentence for each of these, so these will also be printed below.

```{r}
print_sentiment_messages <- function(df) {
  apply(df, 1, function(row) {
    message <- paste("Sentence:", 
                     row["full_sentence"], 
                     "\nSentiment Score:", 
                     row["sentiment"])
    cat(message, "\n\n")
  })
}

print_sentiment_messages(df_top_bottom)
```
